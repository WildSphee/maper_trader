{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6b1cc-05b3-4c6e-9820-40a27bd6da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target window (UTC): 2023-09-28 → 2025-09-27\n",
      "Base save directory: '/home/azureuser/maper_trader/C:\\Users\\yoonc\\Jupyter Notebook'\n",
      "[INFO] n-unique-addresses: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]             n-unique-addresses | saved_as=   day | rows=731\n",
      "[INFO] n-transactions: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                 n-transactions | saved_as=   day | rows=731\n",
      "[INFO] transactions-per-second: chunked OK | reported=minute | saved_as=15min | rows=2017\n",
      "[OK]        transactions-per-second | saved_as= 15min | rows=2017\n",
      "[INFO] output-volume: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                  output-volume | saved_as=   day | rows=731\n",
      "[INFO] mempool-count: chunked OK | reported=minute | saved_as=15min | rows=2019\n",
      "[OK]                  mempool-count | saved_as= 15min | rows=2019\n",
      "[INFO] mempool-growth: chunked OK | reported=minute | saved_as=15min | rows=2017\n",
      "[OK]                 mempool-growth | saved_as= 15min | rows=2017\n",
      "[INFO] mempool-size: chunked OK | reported=minute | saved_as=15min | rows=2019\n",
      "[OK]                   mempool-size | saved_as= 15min | rows=2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1352958/885286975.py:154: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  resampler = sdf.resample(rule, label=\"right\", closed=\"right\", origin=\"start_day\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] utxo-count: chunked OK | reported=hour | saved_as=6h | rows=2924\n",
      "[OK]                     utxo-count | saved_as=    6h | rows=2924\n",
      "[INFO] estimated-transaction-volume-usd: chunked OK | reported=day | saved_as=day | rows=727\n",
      "[OK] estimated-transaction-volume-usd | saved_as=   day | rows=727\n",
      "[WARN] mvrv: 404 Not Found — skipping.\n",
      "[WARN] mvrv: 404 Not Found — skipping.\n",
      "[WARN]                           mvrv (mvrv): no data — skipped\n",
      "[WARN] nvt: 404 Not Found — skipping.\n",
      "[WARN] nvt: 404 Not Found — skipping.\n",
      "[WARN]                            nvt (nvt): no data — skipped\n",
      "[WARN] nvts: 404 Not Found — skipping.\n",
      "[WARN] nvts: 404 Not Found — skipping.\n",
      "[WARN]                           nvts (nvts): no data — skipped\n",
      "[INFO] n-transactions-per-block: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]       n-transactions-per-block | saved_as=   day | rows=731\n",
      "[INFO] median-confirmation-time: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]       median-confirmation-time | saved_as=   day | rows=731\n",
      "[INFO] avg-confirmation-time: chunked OK | reported=6hours | saved_as=day | rows=731\n",
      "[OK]          avg-confirmation-time | saved_as=   day | rows=731\n",
      "[INFO] n-transactions-total: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]           n-transactions-total | saved_as=   day | rows=731\n",
      "[INFO] hash-rate: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                      hash-rate | saved_as=   day | rows=731\n",
      "[INFO] difficulty: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                     difficulty | saved_as=   day | rows=731\n",
      "[INFO] miners-revenue: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                 miners-revenue | saved_as=   day | rows=731\n",
      "[INFO] transaction-fees: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]               transaction-fees | saved_as=   day | rows=731\n",
      "[WARN] fees-usd-per-transaction: 404 Not Found — skipping.\n",
      "[WARN]       fees-usd-per-transaction (fees_usd_per_tx): no data — skipped\n",
      "[INFO] cost-per-transaction-percent: chunked OK | reported=day | saved_as=day | rows=727\n",
      "[OK]   cost-per-transaction-percent | saved_as=   day | rows=727\n",
      "[INFO] blocks-size: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                    blocks-size | saved_as=   day | rows=731\n",
      "[INFO] avg-block-size: chunked OK | reported=day | saved_as=day | rows=731\n",
      "[OK]                 avg-block-size | saved_as=   day | rows=731\n",
      "[WARN] Could not save Parquet for day data: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "[SAVE] Merged day data to: C:\\Users\\yoonc\\Jupyter Notebook/day/btc_merged_from_2023-09-28_to_2025-09-27.csv | cols=15 | rows=731\n",
      "[WARN] Could not save Parquet for 15min data: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "[SAVE] Merged 15min data to: C:\\Users\\yoonc\\Jupyter Notebook/15min/btc_merged_from_2023-09-28_to_2025-09-27.csv | cols=4 | rows=2019\n",
      "[WARN] Could not save Parquet for 6h data: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "[SAVE] Merged 6h data to: C:\\Users\\yoonc\\Jupyter Notebook/6h/btc_merged_from_2023-09-28_to_2025-09-27.csv | cols=1 | rows=2924\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# -------------------- Configuration --------------------------------------------------\n",
    "SAVE_DIR = r\"\\temp\"\n",
    "\n",
    "TODAY = datetime.now(timezone.utc).date()\n",
    "END_DATE = TODAY - timedelta(days=1)                  # yesterday (UTC)\n",
    "START_DATE = END_DATE - timedelta(days=730)           # ~2 years back\n",
    "WINDOW_DAYS = 360\n",
    "\n",
    "# Robustness settings\n",
    "MIN_GOOD_ROWS = 180\n",
    "AGG_INTRADAY_TO_DAILY = False\n",
    "AGG_METHOD = \"mean\"           # \"mean\" or \"last\"\n",
    "\n",
    "# Frequency normalization toggles\n",
    "FORCE_MINUTE_TO_15MIN = True  # snap minute series that are actually on 00/15/30/45 to 15min bins\n",
    "COALESCE_HOURLY_TO_6H = True  # resample hourly series to clean 6-hour bins (UTC anchored)\n",
    "\n",
    "# -------------------- API & Metrics --------------------------------------------------\n",
    "BASE = \"https://api.blockchain.info/charts/{slug}\"\n",
    "UA = {\"User-Agent\": \"onchain-collector/2.6 (+academic use)\"}\n",
    "\n",
    "NETWORK_ACTIVITY = {\n",
    "    \"n-unique-addresses\": \"unique_addresses_used\",\n",
    "    \"n-transactions\": \"confirmed_tx_per_day\",\n",
    "    \"transactions-per-second\": \"tx_rate_per_second\",\n",
    "    \"output-volume\": \"output_value_per_day_btc\",\n",
    "    \"mempool-count\": \"mempool_tx_count\",\n",
    "    \"mempool-growth\": \"mempool_size_growth_bytes\",\n",
    "    \"mempool-size\": \"mempool_size_bytes\",\n",
    "    \"utxo-count\": \"utxo_count\",\n",
    "    \"estimated-transaction-volume-usd\": \"estimated_tx_value_usd\"    \n",
    "}\n",
    "MARKET_SIGNALS = {\"mvrv\": \"mvrv\", \"nvt\": \"nvt\", \"nvts\": \"nvts\"}\n",
    "BLOCK_DETAILS = {\n",
    "    \"n-transactions-per-block\": \"avg_tx_per_block\",\n",
    "    \"median-confirmation-time\": \"median_confirmation_time_min\",\n",
    "    \"avg-confirmation-time\": \"avg_confirmation_time_min\",\n",
    "    \"n-transactions-total\": \"total_tx_count\"\n",
    "}\n",
    "MINING_INFO = {\n",
    "    \"hash-rate\": \"total_hash_rate_ths\",\n",
    "    \"difficulty\": \"network_difficulty\",\n",
    "    \"miners-revenue\": \"miners_revenue_usd\",\n",
    "    \"transaction-fees\": \"total_tx_fees_btc\",\n",
    "    \"fees-usd-per-transaction\": \"fees_usd_per_tx\",\n",
    "    \"cost-per-transaction-percent\": \"cost_pct_of_tx_volume\",\n",
    "    \"blocks-size\": \"blockchain_size\",   \n",
    "    \"avg-block-size\": \"avg_block_size\"\n",
    "}\n",
    "\n",
    "METRICS: Dict[str, str] = {}\n",
    "for group in (NETWORK_ACTIVITY, MARKET_SIGNALS, BLOCK_DETAILS, MINING_INFO):\n",
    "    METRICS.update(group)\n",
    "\n",
    "PREFER_TIMESPAN = {\"mvrv\", \"nvt\", \"nvts\"}\n",
    "\n",
    "# -------------------- HTTP helper ----------------------------------------------------\n",
    "def fetch_chart(slug: str, params: Dict[str, str]) -> Optional[Dict[str, Any]]:\n",
    "    url = BASE.format(slug=slug)\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=UA, timeout=30)\n",
    "        if r.status_code == 404:\n",
    "            print(f\"[WARN] {slug}: 404 Not Found — skipping.\")\n",
    "            return None\n",
    "        if r.status_code == 429:\n",
    "            print(f\"[WARN] {slug}: 429 Too Many Requests — pausing 10s and retrying once.\")\n",
    "            time.sleep(10)\n",
    "            r = requests.get(url, params=params, headers=UA, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR] {slug}: Network request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------- Transform helpers ---------------------------------------------\n",
    "def to_df_with_meta(payload: Dict[str, Any], value_col: str) -> Tuple[pd.DataFrame, str]:\n",
    "    period = payload.get(\"period\", \"\")\n",
    "    vals = payload.get(\"values\", [])\n",
    "    if not vals:\n",
    "        return pd.DataFrame(columns=[\"ts\", value_col]), period\n",
    "    df = pd.DataFrame(vals)\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"x\"], unit=\"s\", utc=True)\n",
    "    df = df.rename(columns={\"y\": value_col})\n",
    "    df = df[[\"ts\", value_col]].sort_values(\"ts\")\n",
    "    return df, period\n",
    "\n",
    "def trim_to_window(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return df[(df[\"ts\"].dt.date >= START_DATE) & (df[\"ts\"].dt.date <= END_DATE)]\n",
    "\n",
    "def dedupe_on_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.index.has_duplicates:\n",
    "        df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    return df\n",
    "\n",
    "def to_daily(df: pd.DataFrame, value_col: str, how: str = \"mean\") -> pd.DataFrame:\n",
    "    sdf = df.set_index(\"ts\").sort_index()\n",
    "    rule = sdf.resample(\"D\")\n",
    "    sdf = rule.mean() if how == \"mean\" else rule.last()\n",
    "    sdf.index.name = \"date\"\n",
    "    # keep rows where the metric exists\n",
    "    if value_col in sdf.columns:\n",
    "        sdf = sdf.dropna(subset=[value_col])\n",
    "    else:\n",
    "        sdf = sdf.dropna(how=\"all\")\n",
    "    return dedupe_on_index(sdf)\n",
    "\n",
    "def to_period_index(df: pd.DataFrame, period: str, value_col: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    if period == \"day\":\n",
    "        out = df.copy()\n",
    "        out[\"date\"] = out[\"ts\"].dt.normalize()\n",
    "        out = out.drop(columns=[\"ts\"]).set_index(\"date\").sort_index()\n",
    "        return dedupe_on_index(out)\n",
    "    else:\n",
    "        out = df.set_index(\"ts\").sort_index()\n",
    "        return dedupe_on_index(out)\n",
    "\n",
    "# ------------ Frequency normalization (fix minute & 6-hour issues) -------------------\n",
    "def _mode_step_seconds(idx: pd.DatetimeIndex) -> Optional[int]:\n",
    "    if len(idx) < 3:\n",
    "        return None\n",
    "    diffs = pd.Series(idx).diff().dropna().dt.total_seconds().astype(int)\n",
    "    if diffs.empty:\n",
    "        return None\n",
    "    # use mode (most frequent delta)\n",
    "    try:\n",
    "        return int(diffs.mode().iloc[0])\n",
    "    except Exception:\n",
    "        return int(diffs.median())\n",
    "\n",
    "def _rule_from_step(step_s: int) -> str:\n",
    "    if step_s % 86400 == 0:\n",
    "        return \"D\"\n",
    "    if step_s % 3600 == 0:\n",
    "        return f\"{step_s // 3600}H\"\n",
    "    if step_s % 60 == 0:\n",
    "        return f\"{step_s // 60}min\"\n",
    "    return f\"{step_s}S\"\n",
    "\n",
    "def _resample(sdf: pd.DataFrame, rule: str, keep_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    agg = \"mean\" if AGG_METHOD == \"mean\" else \"last\"\n",
    "    # anchor bins at UTC midnight to avoid “mixed” intervals\n",
    "    resampler = sdf.resample(rule, label=\"right\", closed=\"right\", origin=\"start_day\")\n",
    "    out = getattr(resampler, agg)()\n",
    "    if keep_cols:\n",
    "        out = out[keep_cols]\n",
    "    out.index.name = \"date\" if rule.upper() == \"D\" else \"ts_utc\"\n",
    "    return out.dropna(how=\"all\")\n",
    "\n",
    "def normalize_frequency(df: pd.DataFrame, reported_period: str, value_col: str) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Ensure a clean, regular grid:\n",
    "      - 'day' stays daily\n",
    "      - 'minute' on 00/15/30/45 => resample to 15min (if FORCE_MINUTE_TO_15MIN)\n",
    "      - 'hour' => coalesce to 6H if enabled\n",
    "      - Otherwise, use modal step across the series\n",
    "    Returns (normalized_df, label_for_folder)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, reported_period or \"unknown\"\n",
    "\n",
    "    # Daily already normalized\n",
    "    if reported_period == \"day\" and (df.index.name == \"date\"):\n",
    "        return df, \"day\"\n",
    "\n",
    "    # Ensure DateTimeIndex\n",
    "    sdf = df.copy()\n",
    "    if not isinstance(sdf.index, pd.DatetimeIndex):\n",
    "        # Expect 'ts' was the index for intraday\n",
    "        if \"ts\" in sdf.columns:\n",
    "            sdf = sdf.set_index(\"ts\").sort_index()\n",
    "        else:\n",
    "            sdf.index = pd.to_datetime(sdf.index, utc=True)\n",
    "\n",
    "    idx = sdf.index\n",
    "\n",
    "    # Detect quarter-hour pattern\n",
    "    is_quarter_marks = (idx.minute % 15 == 0).all() and (idx.second == 0).all()\n",
    "\n",
    "    if reported_period == \"minute\" and FORCE_MINUTE_TO_15MIN and is_quarter_marks:\n",
    "        cleaned = _resample(sdf, \"15min\", keep_cols=[value_col] if value_col in sdf.columns else None)\n",
    "        return cleaned, \"15min\"\n",
    "\n",
    "    if reported_period == \"hour\" and COALESCE_HOURLY_TO_6H:\n",
    "        cleaned = _resample(sdf, \"6H\", keep_cols=[value_col] if value_col in sdf.columns else None)\n",
    "        return cleaned, \"6h\"\n",
    "\n",
    "    # Fallback: infer modal step and resample to that exact rule\n",
    "    step_s = _mode_step_seconds(idx)\n",
    "    if step_s is None:\n",
    "        # nothing to normalize; keep as-is, label by reported_period\n",
    "        return sdf, (reported_period or \"intraday\")\n",
    "    rule = _rule_from_step(step_s)\n",
    "    cleaned = _resample(sdf, rule, keep_cols=[value_col] if value_col in sdf.columns else None)\n",
    "\n",
    "    # Pretty label (e.g., \"1H\"->\"hour\", \"5min\"->\"5min\")\n",
    "    if rule == \"D\":\n",
    "        label = \"day\"\n",
    "    elif rule.endswith(\"H\") and rule != \"6H\":\n",
    "        # 1H -> hour ; 2H -> 2h, etc.\n",
    "        label = \"hour\" if rule == \"1H\" else rule.lower()\n",
    "    else:\n",
    "        label = rule.lower()\n",
    "\n",
    "    return cleaned, label\n",
    "\n",
    "# -------------------- Fetch strategies ----------------------------------------------\n",
    "def pull_timespan(slug: str, col: str) -> Tuple[pd.DataFrame, str]:\n",
    "    payload = fetch_chart(slug, {\"timespan\": \"2years\", \"format\": \"json\", \"sampled\": \"false\"})\n",
    "    if payload is None:\n",
    "        return pd.DataFrame(columns=[col]), \"\"\n",
    "    raw_df, period = to_df_with_meta(payload, col)\n",
    "    raw_df = trim_to_window(raw_df)\n",
    "    if raw_df.empty:\n",
    "        return pd.DataFrame(columns=[col]), period\n",
    "\n",
    "    if AGG_INTRADAY_TO_DAILY and period != \"day\":\n",
    "        out = to_daily(raw_df, col, how=AGG_METHOD)\n",
    "        print(f\"[INFO] {slug}: timespan OK | period={period or '?'} aggregated→day | rows={len(out)}\")\n",
    "        return out, \"day\"\n",
    "\n",
    "    out = to_period_index(raw_df, period, col)\n",
    "    # ---- normalize frequency for intraday series\n",
    "    out, label = normalize_frequency(out, period, col)\n",
    "    print(f\"[INFO] {slug}: timespan OK | reported={period or '?'} | saved_as={label} | rows={len(out)}\")\n",
    "    return out, label\n",
    "\n",
    "def pull_chunked_then_timespan(slug: str, col: str) -> Tuple[pd.DataFrame, str]:\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    periods: List[str] = []\n",
    "    cur_start = START_DATE\n",
    "\n",
    "    while cur_start <= END_DATE:\n",
    "        params = {\"start\": cur_start.strftime(\"%Y-%m-%d\"), \"format\": \"json\", \"sampled\": \"false\"}\n",
    "        payload = fetch_chart(slug, params)\n",
    "        if payload is None:\n",
    "            return pd.DataFrame(columns=[col]), \"\"\n",
    "\n",
    "        part_df, period = to_df_with_meta(payload, col)\n",
    "        if not part_df.empty:\n",
    "            frames.append(part_df)\n",
    "            periods.append(period or \"\")\n",
    "\n",
    "        cur_start = cur_start + timedelta(days=WINDOW_DAYS)\n",
    "        time.sleep(0.35)\n",
    "\n",
    "    if not frames:\n",
    "        return pull_timespan(slug, col)\n",
    "\n",
    "    df_all = pd.concat(frames, axis=0).sort_values(\"ts\")\n",
    "    df_all = trim_to_window(df_all)\n",
    "\n",
    "    if len(df_all) >= MIN_GOOD_ROWS:\n",
    "        base = to_period_index(df_all, periods[0] if periods else \"\", col)\n",
    "        # ---- normalize frequency (fix min and 6h bins)\n",
    "        base, label = normalize_frequency(base, periods[0] if periods else \"\", col)\n",
    "        print(f\"[INFO] {slug}: chunked OK | reported={periods[0] if periods else '?'} | saved_as={label} | rows={len(base)}\")\n",
    "        return base, label\n",
    "\n",
    "    print(f\"[INFO] {slug}: chunked short/mixed (periods={set(periods)}, rows={len(df_all)}). Falling back to timespan.\")\n",
    "    return pull_timespan(slug, col)\n",
    "\n",
    "def pull_metric(slug: str, col: str) -> Tuple[pd.DataFrame, str]:\n",
    "    if slug in PREFER_TIMESPAN:\n",
    "        df, period = pull_timespan(slug, col)\n",
    "        if not df.empty:\n",
    "            return df, period\n",
    "        return pull_chunked_then_timespan(slug, col)\n",
    "    else:\n",
    "        return pull_chunked_then_timespan(slug, col)\n",
    "\n",
    "# -------------------- Main -----------------------------------------------------------\n",
    "def main(save_per_metric: bool = False) -> Dict[str, pd.DataFrame]:\n",
    "    print(f\"Target window (UTC): {START_DATE} → {END_DATE}\")\n",
    "    print(f\"Base save directory: '{os.path.abspath(SAVE_DIR)}'\")\n",
    "\n",
    "    by_label: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for slug, col in METRICS.items():\n",
    "        df, label = pull_metric(slug, col)\n",
    "        if df.empty or not label:\n",
    "            print(f\"[WARN] {slug:>30s} ({col}): no data — skipped\")\n",
    "            continue\n",
    "\n",
    "        if save_per_metric:\n",
    "            period_dir = os.path.join(SAVE_DIR, label)\n",
    "            os.makedirs(period_dir, exist_ok=True)\n",
    "            idx_name = df.index.name or \"ts_utc\"\n",
    "            tmp = df.reset_index().rename(columns={idx_name: \"timestamp_or_date\"})\n",
    "            filename = f\"{slug}_from_{START_DATE}_to_{END_DATE}.csv\"\n",
    "            filepath = os.path.join(period_dir, filename)\n",
    "            tmp.to_csv(filepath, index=False, float_format=\"%.10g\")\n",
    "\n",
    "        by_label.setdefault(label, []).append(df.rename(columns={df.columns[0]: col}))\n",
    "        print(f\"[OK] {slug:>30s} | saved_as={label:>6s} | rows={len(df)}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    if not by_label:\n",
    "        raise RuntimeError(\"No non-empty series collected. Check network or slugs.\")\n",
    "\n",
    "    merged_by_label: Dict[str, pd.DataFrame] = {}\n",
    "    for label, frames in by_label.items():\n",
    "        period_dir = os.path.join(SAVE_DIR, label)\n",
    "        os.makedirs(period_dir, exist_ok=True)\n",
    "\n",
    "        merged = pd.concat(frames, axis=1, join=\"outer\", sort=True).dropna(how=\"all\")\n",
    "        merged_by_label[label] = merged\n",
    "\n",
    "        base_filename = f\"btc_merged_from_{START_DATE}_to_{END_DATE}\"\n",
    "        csv_path = os.path.join(period_dir, f\"{base_filename}.csv\")\n",
    "        pq_path = os.path.join(period_dir, f\"{base_filename}.parquet\")\n",
    "\n",
    "        index_label = \"date\" if label == \"day\" else \"ts_utc\"\n",
    "        merged.to_csv(csv_path, float_format=\"%.10g\", index_label=index_label)\n",
    "\n",
    "        try:\n",
    "            merged.to_parquet(pq_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not save Parquet for {label} data: {e}\")\n",
    "\n",
    "        print(f\"[SAVE] Merged {label} data to: {csv_path} | cols={len(merged.columns)} | rows={len(merged)}\")\n",
    "\n",
    "    return merged_by_label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_sets = main(save_per_metric=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d23a08-c973-4b41-b3b1-b28a3dd19349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab310-81df-4ce9-9973-430c17aff0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57669b0-6d01-4d35-903c-b42d2eac0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
